---
title: "Package functions"
author: "Fonti Kar"
date: "2023-04-27"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preamble

This document maps out the key steps for converting a `.csv` data file into a Darwin Core XML file. These steps will become eventual functions for this R package.

## Packages/dependencies we need

```{r}
pacman::p_load(tidyverse, here, skimr, janitor)
```

## Example data

I retrieved some example data from [Dyrad](https://datadryad.org/stash/dataset/doi:10.5061%2Fdryad.j9kd51cgr), the respective publication found [here](https://onlinelibrary.wiley.com/doi/full/10.1111/gcb.16501). This study looks like compiled unpublished and unpublished as well collected field data e.g ("A. Westerband" in References field are field collected and presumably unpublished). 

1. Create a folder within the package and name it 'ignore'. This folder is not detected by git and will not be pushed to the repo.
2. Download the zip folder and unzip into `ignore/`.
3. Save the enclosed `.xlsx` file as a `.csv` (manual step)
```{r}
# Read in data
westerband_2022 <- readr::read_csv(here("ignore/doi_10.5061_dryad.j9kd51cgr__v4/Westerband_2022_GCB_DRYAD.csv"))

#Preview
dplyr::glimpse(westerband_2022)
skimr::skim(westerband_2022)
```

Generate some random dates

```{r}
westerband_2022 <- westerband_2022 |> 
  mutate(Collection_date = sample(seq(as.Date('2022/01/01'), as.Date('2023/01/01'), by="day"), nrow(westerband_2022), replace = TRUE))

# write_csv(westerband_2022, here("data/westerband_2022_wdate.csv"))

# Testing what happens with readr::read_csv when you read in a .csv that contains dates. Does it parse as string? 
westerband_2022_wdate <- read_csv(here("data/westerband_2022_wdate.csv"))

glimpse(westerband_2022_wdate) # Yes it guesses the date since it is formatted as yyyy-mm-dd

# What if dates are in non-POSIT formats? See # Checking dates
```

### 1) Assign a unique identifier

```{r}
# https://samfirke.com/2018/08/22/generating-unique-ids-using-r/
create_unique_ids <- function(n, seed_no = 1, char_len = 5){
  set.seed(seed_no)
  pool <- c(letters, LETTERS, 0:9)
  
  res <- character(n) # pre-allocating vector is much faster than growing it
  for(i in seq(n)){
    this_res <- paste0(sample(pool, char_len, replace = TRUE), collapse = "")
    while(this_res %in% res){ # if there was a duplicate, redo
      this_res <- paste0(sample(pool, char_len, replace = TRUE), collapse = "")
    }
      res[i] <- this_res
  }
  res
}

westerband_2022_wdate |> 
  mutate(occurrenceID = create_unique_ids(nrow(westerband_2022))) 
```


### 2) Identify relevant columns

Relying on the ALA [data ingestion documentation](https://support.ala.org.au/support/solutions/articles/6000261427-sharing-a-dataset-with-the-ala) here to choose relevant columns

#### a - Species name

```{r}
potential_names <- c("species",  "SPECIES",
                     "name", "NAME",
                     "species_name", "speciesName", "SPECIES_NAME",
                     "scientific_name", "scientificName", "SCIENTIFIC_NAME",
                     "latin_name", "latinName", "LATIN_NAME",
                     "valid_name", "validName", "VALID_NAME", 
                     "complete_name", "completeName" ,"COMPLETE_NAME"
                      )

# String matching
# str_which(potential_names, pattern = regex("species|name|scientific", ignore_case = TRUE))
str_which(names(westerband_2022_wdate), pattern = regex("species|name|scientific", ignore_case = TRUE))

# Select the matched column
species <- westerband_2022_wdate |> 
  select(str_which(names(westerband_2022_wdate), pattern = regex("species|name|scientific", ignore_case = TRUE))) 

# Rename as scientificName
species <- species |> 
  rename(raw_scientificName = Species) 
```
#### b - Date information

```{r}
# Select date column
date <- select_if(westerband_2022_wdate, function(x) inherits(x, 'Date'))

# Rename as raw_eventDate
date <- date |> 
  rename(raw_eventDate = Collection_date) 
```

##### Checking dates

What if date information is inputted in other formats e.g 1-Jul-1998, 20-Mar-23, 06-30-2018 (June 30th - North American formatting) 
We will need to split the string up and guess if its a month, day, year base on number of characters and ranges of the column.

Wonder if we can use {pointblank} for these checks

```{r}
# Example 1
date_1 <- tibble(date = c("1-Jul-1998", "1-Aug-1998", "1-Sept-1998", "1-Oct-1998"))
date_1

# Example 2 d/m/yyyy
date_2 <- tibble(date = c("1/7/1998", "1/8/1998", "1/9/1998", "1/10/1998"))
date_2

# Example 3 m.d.yyyy
date_3 <- tibble(date = c("7.1.1998", "8.9.1998", "9.15.1998", "10.27.1998"))
date_3

#Split string by common date seperators - / . 
date_separators <- c("-", "/", "\\.")

# Separate date to guess date format
split_date <- date_1 |> 
  separate_wider_delim(cols = date,
                       delim = regex(paste(date_separators, collapse = "|")),
                       names = c("first", "second", "third"))

# Which column contains letters?
map_lgl(split_date,
    ~any(str_detect(.x, pattern = regex("[:alpha:]")) ))

letters_res <- map_lgl(split_date,
    ~any(str_detect(.x, pattern = regex("[:alpha:]")) ))

# Letters column is likely to be month
# Detect month letter strings
month_letter_pattern <-  c("Jan", "Feb", "Mar", "Apr", "May", "Jun",
                           "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")

str_detect(split_date[letters_res] |> pull() , pattern = regex(paste(month_letter_pattern, collapse = "|"))) |> any()

# Mutate column as month
split_date |> 
  mutate(month = split_date[letters_res] |> pull())

# Which column contains digits?
digit_res <- map_lgl(split_date,
    ~any(str_detect(.x, pattern = regex("[:digit:]")) ))


# How many characters do values in each column have?
map_vec(split_date[which(digit_res)],
        ~ nchar(.x) |> unique())

# Likely year if 4 characters
year_char_test <- map_vec(split_date[which(digit_res)],
        ~ nchar(.x) |> unique() == 4)

split_date[which(year_char_test == TRUE)]

# If digit, convert to numeric
# Test if values are between 1-12
map(split_date[which(digit_res)],
        ~ as.numeric(.x))

map_vec(split_date[which(digit_res)],
    ~ as.numeric(max(.x)) <= 31)

# Check if max is less than 12, if FALSE then its a month
split_date$first |> as.numeric() |> max() > 12
```

#### c - Location information

Should site here be...locationRemarks? or verbatimLocality? 

```{r}
#site, collection_site, location, character, state, country, latitude, longitude

location <- westerband_2022_wdate |> 
  select(str_which(names(westerband_2022), pattern = regex("site|location|state|country|province|latitude|longitude", ignore_case = TRUE))) 

# Rename columns respectively 
location <- location |> 
  rename(raw_decimalLatitude =  Latitude,
         raw_decimaleLongitude = Longitude,
         verbatimLocality = Site)
```

##### Check location information

Presumably the user needs to filter some of these sites out? Some of these are plant genuses e.g Transcontinentalis
Maybe a check box that users can click to include in data? 

```{r}
location$verbatimLocality |> tabyl()
```

### 3) Put seperate datasets together 

Just the bare minimum data here

```{r}
westerband_subset <- bind_cols(species, date, location)
```

### 4) Export as .csv

```{r}
write_csv(westerband_subset, here("output/csv/westerband_subset.csv"))
```

### 5) Meta data

Currently made for mandatory fields only, template found [here](https://support.ala.org.au/support/solutions/articles/6000261427-sharing-a-dataset-with-the-ala#minimum-fields)

```{r}
knitr::purl(here("doc/metadata.Rmd"), output=here("output/script/metadata.R"), docmentation = 1)
```

